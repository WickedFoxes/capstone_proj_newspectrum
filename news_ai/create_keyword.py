# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
from model import *

import uuid
from datetime import datetime, timedelta

# NLP ëª¨ë¸
from keybert import KeyBERT
from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification, pipeline
from konlpy.tag import Okt, Komoran, Kkma

# KoSimCSE-roberta ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
koSimCSE_model = AutoModel.from_pretrained("BM-K/KoSimCSE-roberta")
# KeyBERTì— KoBERT ëª¨ë¸ ì—°ê²°
kw_model = KeyBERT(model=koSimCSE_model)

# tokenizer
# tag = Okt()
tag = Komoran(userdic='user_dict.txt')
# tag = Kkma()

korean_stopwords = [
    'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ê·¸ë˜ì„œ', 'ë˜í•œ',
    'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë•Œë¬¸ì—', 'ë°',
    'í•˜ëŠ”', 'í•˜ì—¬', 'í•˜ê²Œ', 'í•˜ê³ ', 'ì´ë‹¤', 'ìˆëŠ”', 'ì—†ëŠ”',
    'ëë‹¤', 'ë˜ì—ˆë‹¤', 'ê°™ì€', 'ìœ„í•œ', 'ëŒ€í•œ', 'ì¤‘ì¸', 'í•˜ë©´ì„œ',
    'ë³´ë‹¤', 'ê°€ì§€', 'ë“±ì˜', 'ì´ë²ˆ', 'ê´€ë ¨', 'í†µí•´', 'ê²½ìš°',
    'ì—ì„œ', 'ìœ¼ë¡œ', 'ê¹Œì§€', 'ë¶€í„°', 'ì—ëŠ”', 'ì—ì„œì˜',
    'í•˜ê²Œ', 'ë˜ë©°', 'ë˜ì–´', 'ë˜ê¸°', 'ëœë‹¤', 'ëë‹¤',
    'ë¼ê³ ', 'ìœ¼ë¡œì„œ', 'ë¡œì„œ', 'ì˜€ë‹¤', 'ì´ì—ˆë‹¤', 'í† ìš”ì™€ì´ë“œ'
]

def extract_keywords_from_article(title, content, top_n=7):
    title = title.replace("\n", "")
    content = content.replace("\n", "")
    input = title+" "+content[:700]

    content_keywords = kw_model.extract_keywords(
            input,
            top_n=top_n, 
            stop_words=korean_stopwords,
            keyphrase_ngram_range=(1, 1),
            use_mmr=True,
            diversity=0.7
    )
    
    return content_keywords

def get_48_hours_before(load_date_str):
    # ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
    load_date = datetime.strptime(load_date_str, "%Y-%m-%d %H:%M:%S")
    
    # 48ì‹œê°„ ì´ì „ ê³„ì‚°
    before_48_hours = load_date - timedelta(hours=48)
    
    # ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ ë°˜í™˜ (ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ)
    return before_48_hours.strftime("%Y-%m-%d %H:%M:%S")


################# ì‚¬ìš©ì ì…ë ¥ í•„ìš” #################
end_date_str = "2025-03-02 00:00:00"
start_date_str = "2025-02-01 00:00:00"
domains = ['ì •ì¹˜','ê²½ì œ','ì‚¬íšŒ','ìŠ¤í¬ì¸ ','ì—°ì˜ˆ']
################# ì…ë ¥ ë ###########################

start_date = datetime.strptime(start_date_str, "%Y-%m-%d %H:%M:%S")
end_date = datetime.strptime(end_date_str, "%Y-%m-%d %H:%M:%S")
delta = timedelta(hours=48)

current_end = start_date
while current_end <= end_date:
    current_start = current_end - delta
    print(f"\nğŸ“… ë‚ ì§œ ë²”ìœ„: {current_start} ~ {current_end}")

    for domain in domains:
        # ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        articles = read_news_articles_by_domain(domain=domain,
                                                date_str=current_start.strftime("%Y-%m-%d %H:%M:%S"),
                                                date_end=current_end.strftime("%Y-%m-%d %H:%M:%S"))
        print(f"  ğŸ“° [{domain}] ë¬¸ì„œ ê°œìˆ˜: {len(articles)}")
        
        # ë‰´ìŠ¤ í‚¤ì›Œë“œ ìƒì„±
        for article in articles:
            items = extract_keywords_from_article(article.title, article.content, top_n=7)
            for keyword, score in items:
                keyword_item = Keyword(id=None,
                                    keyword=keyword,
                                    score=score,
                                    created_date=current_end,
                                    news_article_id=article.id)
                create_news_keyword(keyword_item)    

    current_end += delta  # 48ì‹œê°„ ì „ì§„
