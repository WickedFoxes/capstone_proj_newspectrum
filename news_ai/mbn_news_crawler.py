import argparse
import requests
import pymysql
import json
from datetime import datetime
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from model import *

section_type = {
    "politics" : "politics",
    "economy" : "economy",
    "society" : "society",
    "sports" : "sports",
    "entertain" : "entertain",
}
section_korean = {
    "politics" : "ì •ì¹˜",
    "economy" : "ê²½ì œ",
    "society" : "ì‚¬íšŒ",
    "sports" : "ìŠ¤í¬ì¸ ",
    "entertain" : "ì—°ì˜ˆ",
}

def is_href_exist(href):
    try:
        conn = pymysql.connect(host='localhost',
                               user='root',
                               password='root',
                               db='newspectrum',
                               charset='utf8mb4')
        cur = conn.cursor()

        sql = "SELECT EXISTS(SELECT 1 FROM news_article WHERE href = %s)"
        cur.execute(sql, (href,))
        result = cur.fetchone()[0]

        return bool(result)

    except pymysql.MySQLError as e:
        print("âŒ DB ì˜¤ë¥˜:", e)
        return False  # ì˜¤ë¥˜ ë°œìƒ ì‹œ False ë¦¬í„´ (í•„ìš”ì— ë”°ë¼ ë³€ê²½ ê°€ëŠ¥)

    finally:
        try:
            if cur:
                cur.close()
            if conn:
                conn.close()
        except:
            pass

def fetch_and_upload_detail(href: str, domain: str):
    try:
        if is_href_exist(href):
            return

        detail_request = requests.get(href, timeout=10)
        bsoup = BeautifulSoup(detail_request.content, 'html.parser')

        # ld+jsonì—ì„œ ì´ë¯¸ì§€ì™€ ë‚ ì§œ ì¶”ì¶œ
        ld_json_tag = bsoup.find('script', type='application/ld+json')
        img_url = ""
        create_date = ""

        if ld_json_tag:
            try:
                data = json.loads(ld_json_tag.string)
                img_url = data["image"][0] if isinstance(data["image"], list) else data["image"]
                datePublished = data["datePublished"]
                dt = datetime.fromisoformat(datePublished).replace(tzinfo=None)
                create_date = dt.strftime('%Y-%m-%d %H:%M:%S')
            except Exception as e:
                print(f"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")

        # ì œëª© ë° ë³¸ë¬¸ ì¶”ì¶œ
        title_tag = bsoup.select_one('div.title_box > div > h1')
        content_tag = bsoup.select_one('#newsViewArea')
        if not title_tag or not content_tag:
            print(f"âš ï¸ íŒŒì‹± ì‹¤íŒ¨: {href}")
            return

        title = title_tag.text.strip()
        content = content_tag.text.strip()

        news_article = NewsArticle(
            title=title.replace('\n', ''),
            content=content,
            domain=section_korean[domain],
            media="mbn",
            href=href,
            img_url=img_url,
            created_date=create_date,
            comics_url=None,
            summary=None
        )
        create_news_article(news_article)
        print(f"âœ… ì—…ë¡œë“œ ì„±ê³µ: {title[:20]}...")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ ({href}): {e}")

def upload_news(domain="ì •ì¹˜", pageStr=1, pageEnd=148):
    for page in range(pageStr, pageEnd+1):
        print(f"[ğŸ“„ í˜ì´ì§€ {page}]")
        board_request = requests.get(f"https://www.mbn.co.kr/news/{section_type[domain]}/?page={page}")
        bsoup = BeautifulSoup(board_request.content, 'html.parser')
        news_list = bsoup.select('div.list_area > dl > dt > a')

        if len(news_list) == 0:
            print("âŒ ë‰´ìŠ¤ ì—†ìŒ, ì¤‘ë‹¨")
            return False

        detail_hrefs = []
        for news in news_list:
            url = news.attrs['href']
            if url.startswith('//'):
                detail_hrefs.append("https:" + url)
            else:
                detail_hrefs.append(f"https://www.mbn.co.kr/news/{section_type[domain]}/{url}")

        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(fetch_and_upload_detail, href, domain) for href in detail_hrefs]
            for _ in as_completed(futures):
                pass

    return True


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="mbn ë‰´ìŠ¤ í¬ë¡¤ëŸ¬")
    parser.add_argument(
        "--domain", type=str, default="ì •ì¹˜",
        choices=list(section_type.keys()),
        help="í¬ë¡¤ë§í•  ë‰´ìŠ¤ ë„ë©”ì¸ (ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ, ìŠ¤í¬ì¸ , ì—°ì˜ˆ)"
    )
    parser.add_argument(
        "--start", type=int, default=1,
        help="ì‹œì‘ í˜ì´ì§€ ë²ˆí˜¸ (ê¸°ë³¸ê°’: 1)"
    )
    parser.add_argument(
        "--end", type=int, default=98,
        help="ë í˜ì´ì§€ ë²ˆí˜¸ (ê¸°ë³¸ê°’: 98)"
    )

    args = parser.parse_args()

    print(f"ğŸ“° ë„ë©”ì¸: {args.domain} | í˜ì´ì§€: {args.start} ~ {args.end}")
    upload_news(domain=args.domain, pageStr=args.start, pageEnd=args.end)