# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
from model import *

import uuid
import numpy as np
from datetime import datetime, timedelta
from collections import Counter

# NLP ëª¨ë¸
from konlpy.tag import Okt, Komoran, Kkma
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import AgglomerativeClustering

# tokenizer
# tag = Okt()
tag = Komoran(userdic='user_dict.txt')
# tag = Kkma()

def compute_tfidf_embedding(contents: list[str]):
    vectorizer = TfidfVectorizer(
        tokenizer=tag.nouns,
        max_df=0.9,
        min_df=2,
    )
    tfidf_matrix = vectorizer.fit_transform(contents)
    feature_names = vectorizer.get_feature_names_out()

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    cosine_sim = cosine_similarity(tfidf_matrix)
    embeddings = 1 - cosine_sim

    return embeddings, tfidf_matrix, feature_names

def cluster_articles(embeddings, n_clusters: int = 9):
    # ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§
    clustering = AgglomerativeClustering(
        n_clusters=n_clusters,
        metric='precomputed',
        linkage='average'
    )
    labels = clustering.fit_predict(embeddings)
    return labels


def get_48_hours_before(load_date_str):
    # ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
    load_date = datetime.strptime(load_date_str, "%Y-%m-%d %H:%M:%S")
    
    # 48ì‹œê°„ ì´ì „ ê³„ì‚°
    before_48_hours = load_date - timedelta(hours=48)
    
    # ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ ë°˜í™˜ (ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ)
    return before_48_hours.strftime("%Y-%m-%d %H:%M:%S")


################# ì‚¬ìš©ì ì…ë ¥ í•„ìš” #################
end_date_str = "2025-03-02 18:00:00"
start_date_str = "2025-02-01 18:00:00"
domains = ['ì •ì¹˜','ê²½ì œ','ì‚¬íšŒ','ìŠ¤í¬ì¸ ','ì—°ì˜ˆ']
################# ì…ë ¥ ë ###########################

start_date = datetime.strptime(start_date_str, "%Y-%m-%d %H:%M:%S")
end_date = datetime.strptime(end_date_str, "%Y-%m-%d %H:%M:%S")
delta = timedelta(hours=48)

current_end = start_date
while current_end < end_date:
    current_start = current_end - delta
    print(f"\nğŸ“… ë‚ ì§œ ë²”ìœ„: {current_start} ~ {current_end}")

    for domain in domains:
        # ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        articles = read_news_articles_by_domain(domain=domain,
                                                date_str=current_start.strftime("%Y-%m-%d %H:%M:%S"),
                                                date_end=current_end.strftime("%Y-%m-%d %H:%M:%S"))
        titles = [article.title for article in articles]
        contents = [article.content for article in articles]
        created_dates = [article.created_date for article in articles]
        print(f"  ğŸ“° [{domain}] ë¬¸ì„œ ê°œìˆ˜: {len(contents)}")

        # í´ëŸ¬ìŠ¤í„° ìƒì„±
        cluster_n = len(contents)*2//5

        embeddings, tfidf_matrix, feature_names = compute_tfidf_embedding(contents)
        print(embeddings.shape, tfidf_matrix.shape, feature_names.shape)

        labels = cluster_articles(embeddings, n_clusters=cluster_n)
        label_counts = Counter(labels)

        for label in label_counts:
            if(label_counts[label] < 3): continue
            print(f"Cluster {label}: {label_counts[label]}ê°œ")
            
            indices = np.where(labels == label)[0]
            print(label, indices)

            cluster_id=uuid.uuid4()
            for idx in indices:
                article = articles[idx]
                news_cluster_item = NewsCluster(id=None,
                                                cluster_id=cluster_id,
                                                news_article_id=article.id,
                                                created_date=current_end)
                create_news_cluster(news_cluster_item)        

    current_end += delta  # 48ì‹œê°„ ì „ì§„
