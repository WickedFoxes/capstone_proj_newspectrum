import requests
import pymysql
import json
from datetime import datetime
from bs4 import BeautifulSoup

sbs_section_type = {
    "ì •ì¹˜" : "01",
    "ê²½ì œ" : "02",
    "ì‚¬íšŒ" : "03",
    "ìŠ¤í¬ì¸ " : "09",
    "ì—°ì˜ˆ" : "14",
}

def upload_to_db(title:str,
                 content:str,
                 domain:str,
                 media:str,
                 href:str,
                 img_url:str,
                 create_date:str):
    try:
        # DB ì—°ê²°
        conn = pymysql.connect(host='localhost',
                            user='root',
                            password='root',
                            db='newspectrum',
                            charset='utf8mb4')
        cur = conn.cursor()

        # SQL êµ¬ë¬¸
        sql = ("INSERT INTO news_article (title, content, domain, media, href, img_url, created_date) "
            "VALUES (%s, %s, %s, %s, %s, %s, %s)")
        values = (title, content, domain, media, href, img_url, create_date)

        # SQL ì‹¤í–‰
        cur.execute(sql, values)

        # ë³€ê²½ì‚¬í•­ ì»¤ë°‹
        conn.commit()
        print("âœ… ë°ì´í„° ì‚½ì… ì„±ê³µ")

    except pymysql.MySQLError as e:
        print("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜ ë°œìƒ:", e)

    except Exception as e:
        print("âŒ ê¸°íƒ€ ì˜ˆì™¸ ë°œìƒ:", e)

    finally:
        # ì»¤ì„œ ë° ì—°ê²° ì¢…ë£Œ
        try:
            if cur:
                cur.close()
            if conn:
                conn.close()
            print("ğŸ”’ ì—°ê²° ì¢…ë£Œ")
        except:
            pass

def is_href_exist(href):
    try:
        conn = pymysql.connect(host='localhost',
                               user='root',
                               password='root',
                               db='newspectrum',
                               charset='utf8mb4')
        cur = conn.cursor()

        sql = "SELECT EXISTS(SELECT 1 FROM news_article WHERE href = %s)"
        cur.execute(sql, (href,))
        result = cur.fetchone()[0]

        return bool(result)

    except pymysql.MySQLError as e:
        print("âŒ DB ì˜¤ë¥˜:", e)
        return False  # ì˜¤ë¥˜ ë°œìƒ ì‹œ False ë¦¬í„´ (í•„ìš”ì— ë”°ë¼ ë³€ê²½ ê°€ëŠ¥)

    finally:
        try:
            if cur:
                cur.close()
            if conn:
                conn.close()
        except:
            pass

def upload_sbs_news(pageDate:str = "20250321", 
                 pageIdx:int = 1, 
                 domain:str = "ì •ì¹˜"):
    board_request = requests.get("https://news.sbs.co.kr/news/newsSection.do?"+
                        f"pageIdx={pageIdx}&sectionType={sbs_section_type[domain]}&pageDate={pageDate}")
    bsoup = BeautifulSoup(board_request.content, 'html.parser')

    news_list = bsoup.select('div.w_news_list > ul > li > a')
    
    
    detail_hrefs = []
    if(len(news_list) == 0): return False
    
    for news in news_list:
        detail_hrefs.append(news.attrs['href'])
    
    for href in detail_hrefs:
        if(is_href_exist(href)): continue
        detail_request = requests.get(href)
        bsoup = BeautifulSoup(detail_request.content, 'html.parser')

        title = bsoup.select_one('#news-title').text  #ëŒ€ê´„í˜¸ëŠ”  h3#articleTitle ì¸ ê²ƒì¤‘ ì²«ë²ˆì§¸ ê·¸ë£¹ë§Œ ê°€ì ¸ì˜¤ê² ë‹¤.
        content = bsoup.select_one('div.main_text > div').text
        
        # ì¸ë„¤ì¼ ì°¾ê¸°
        img_url = ""
        ld_json_tag = bsoup.find('script', type='application/ld+json')
        
        # í•´ë‹¹ íƒœê·¸ ì•ˆì˜ JSON ë¬¸ìì—´ íŒŒì‹±
        if ld_json_tag:
            try:
                data = json.loads(ld_json_tag.string)
                # thumbnailUrl ì¶”ì¶œ
                img_url = data["image"][0]
            except:
                print("image íŒŒì‹± ì˜¤ë¥˜")
        create_date = bsoup.select_one('div.date_area > span:nth-child(2)').text

        upload_to_db(title=title,
                     content=content,
                     domain=domain,
                     media="sbs",
                     href=href,
                     img_url=img_url,
                     create_date=create_date)
    return True


# SBS-2025.03
for date in range(20250301, 20250405):
    for page in range(1, 10):
        upload_sbs_news(pageDate=str(date),pageIdx=str(page),domain="ì •ì¹˜")
        upload_sbs_news(pageDate=str(date),pageIdx=str(page),domain="ê²½ì œ")
        upload_sbs_news(pageDate=str(date),pageIdx=str(page),domain="ì‚¬íšŒ")
        upload_sbs_news(pageDate=str(date),pageIdx=str(page),domain="ìŠ¤í¬ì¸ ")
        upload_sbs_news(pageDate=str(date),pageIdx=str(page),domain="ì—°ì˜ˆ")

# # SBS-2025.02
# for date in range(20250201, 20250228+1):
#     for page in range(1, 10):
#         upload_sbs_news(pageDate=str(date),pageIdx=str(page),type="ì •ì¹˜")
#         upload_sbs_news(pageDate=str(date),pageIdx=str(page),type="ê²½ì œ")
#         upload_sbs_news(pageDate=str(date),pageIdx=str(page),type="ì‚¬íšŒ")
#         upload_sbs_news(pageDate=str(date),pageIdx=str(page),type="ìŠ¤í¬ì¸ ")
#         upload_sbs_news(pageDate=str(date),pageIdx=str(page),type="ì—°ì˜ˆ")

# # SBS-2025.01
# for date in range(20250101, 20250131+1):
#     for page in range(1, 10):
#         upload_sbs_news(pageDate=str(date),pageIdx=str(page),type="ì •ì¹˜")
#         upload_sbs_news(pageDate=str(date),pageIdx=str(page),type="ê²½ì œ")
#         upload_sbs_news(pageDate=str(date),pageIdx=str(page),type="ì‚¬íšŒ")
#         upload_sbs_news(pageDate=str(date),pageIdx=str(page),type="ìŠ¤í¬ì¸ ")
#         upload_sbs_news(pageDate=str(date),pageIdx=str(page),type="ì—°ì˜ˆ")